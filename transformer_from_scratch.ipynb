{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ9pYd7rdPfa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paste the below code to console to prevent runtime from unconnecting."
      ],
      "metadata": {
        "id": "_rX0fHa4xg86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "var startClickConnect = function startClickConnect() {\n",
        "  var clickConnect = function clickConnect() {\n",
        "    console.log(\"Connect button has been clicked.\");\n",
        "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click();\n",
        "  };\n",
        "\n",
        "  var intervalId = setInterval(clickConnect, 60000);\n",
        "\n",
        "  var stopClickConnectHandler = function stopClickConnect() {\n",
        "    clearInterval(intervalId);\n",
        "    console.log(\"Stop auto clicker.\");\n",
        "  };\n",
        "\n",
        "  return stopClickConnectHandler;\n",
        "};\n",
        "\n",
        "var stopClickConnect = startClickConnect();\n",
        "```\n"
      ],
      "metadata": {
        "id": "sizEVHCcxAJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooV5SiqXiR9j"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch triton\n",
        "!pip install wandb\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUpk9TAXeP3U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import pdb\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchmetrics\n",
        "from torchmetrics.text import BLEUScore\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "\n",
        "from transformers import BertTokenizer, PreTrainedTokenizerFast, DataCollatorForSeq2Seq\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_1xq1z3Xj7M"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "vocab_size = 30000\n",
        "max_len = 20\n",
        "pad_id = 0\n",
        "ukn_id = 1\n",
        "bos_id = 2\n",
        "eos_id = 3\n",
        "\n",
        "# Model\n",
        "n_encoder_layers = 6\n",
        "n_decoder_layers = 6\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "d_ff = 2048\n",
        "dropout = 0.1\n",
        "\n",
        "# Train\n",
        "batch_size = 512\n",
        "n_epochs = 50\n",
        "warmup_steps = 4000\n",
        "\n",
        "colab_dir = '/content/drive/My Drive/Colab Notebooks/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2idmxIcWV78"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"vocab_size\": vocab_size,\n",
        "    \"max_len\": max_len,\n",
        "    \"pad_id\": pad_id,\n",
        "    \"ukn_id\": ukn_id,\n",
        "    \"bos_id\": bos_id,\n",
        "    \"eos_id\": eos_id,\n",
        "    \"n_encoder_layers\": n_encoder_layers,\n",
        "    \"n_decoder_layers\": n_decoder_layers,\n",
        "    \"d_model\": d_model,\n",
        "    \"n_heads\": n_heads,\n",
        "    \"d_ff\": d_ff,\n",
        "    \"dropout\": dropout,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"n_epochs\": n_epochs,\n",
        "    \"warmup_steps\": warmup_steps\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FkWCsTYejN9"
      },
      "outputs": [],
      "source": [
        "file_path = os.path.join(colab_dir, 'dataset/eng-fra.csv')\n",
        "df = pd.read_csv(file_path, encoding='utf-8')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE5t_yOJe3p3"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(df)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8z47jCWgcQd"
      },
      "outputs": [],
      "source": [
        "split_dataset = dataset.train_test_split(test_size=0.4, shuffle=True)\n",
        "train_dataset = split_dataset['train']\n",
        "temp_dataset = split_dataset['test']\n",
        "\n",
        "split_temp_dataset = temp_dataset.train_test_split(test_size=0.5, shuffle=False)\n",
        "val_dataset = split_temp_dataset['train']\n",
        "test_dataset = split_temp_dataset['test']\n",
        "\n",
        "print(\"Train dataset size:\", len(train_dataset))\n",
        "print(\"Validation dataset size:\", len(val_dataset))\n",
        "print(\"Test dataset size:\", len(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg4JAk1Rh2HE"
      },
      "outputs": [],
      "source": [
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "def batch_iterator():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"English words/sentences\"]\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"French words/sentences\"]\n",
        "\n",
        "tokenizer.train_from_iterator(batch_iterator(),\n",
        "                              vocab_size=vocab_size,\n",
        "                              length=len(dataset['English words/sentences']) + len(dataset['French words/sentences']),\n",
        "                              special_tokens=[\"[PAD]\", \"[UNK]\", \"<s>\",\"</s>\"])\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"<s> $A </s>\",\n",
        "    special_tokens=[\n",
        "        (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        "        (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    ]\n",
        ")\n",
        "\n",
        "tokenizer_save_path = \"pretrained-bpe-tokenizer\"\n",
        "os.makedirs(tokenizer_save_path, exist_ok=True)\n",
        "\n",
        "tokenizer_json_path = os.path.join(tokenizer_save_path, \"tokenizer.json\")\n",
        "tokenizer.save(tokenizer_json_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Yv5SIUhwF7U"
      },
      "outputs": [],
      "source": [
        "# Wrap the trained tokenizer with PreTrainedTokenizerFast\n",
        "new_tokenizer = PreTrainedTokenizerFast(tokenizer_file=tokenizer_json_path)\n",
        "\n",
        "new_tokenizer.add_special_tokens({\n",
        "    'pad_token': '[PAD]',\n",
        "    'unk_token': '[UNK]',\n",
        "    'bos_token': '<s>',\n",
        "    'eos_token': '</s>',\n",
        "})\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return new_tokenizer(\n",
        "        examples['English words/sentences'],\n",
        "        text_target=examples['French words/sentences'],\n",
        "        max_length=max_len, # 99.38% of sentences have 20 tokens or less\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt',\n",
        "        return_token_type_ids=False,\n",
        "    )\n",
        "\n",
        "# Apply tokenization to the datasets\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Remove the original text columns from the tokenized datasets\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['English words/sentences', 'French words/sentences'])\n",
        "tokenized_val_dataset = tokenized_val_dataset.remove_columns(['English words/sentences', 'French words/sentences'])\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['English words/sentences', 'French words/sentences'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoNWYDQluQaC"
      },
      "outputs": [],
      "source": [
        "print(tokenized_train_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H93Ru81zFhYZ"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.get_vocab_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zOva3uXCeaB"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, dropout: float=0.1, max_len: int=512):\n",
        "    super().__init__()\n",
        "    assert d_model % 2 == 0, \"d_model must be even\"\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    position_idx = torch.arange(max_len).unsqueeze(1) # [max_len, 1]\n",
        "    dim_idx = torch.arange(0, d_model, 2).unsqueeze(0) # [1, d_model//2]\n",
        "    den = torch.exp(-dim_idx/d_model * math.log(10000)) # [1, d_model//2]\n",
        "\n",
        "    positional_encoding = torch.zeros(max_len, d_model)\n",
        "    positional_encoding[:, 0::2] = torch.sin(position_idx * den) # even\n",
        "    positional_encoding[:, 1::2] = torch.cos(position_idx * den) # odd\n",
        "    positional_encoding = positional_encoding.unsqueeze(0) # [1, max_len, d_model]\n",
        "\n",
        "    self.register_buffer('positional_encoding', positional_encoding)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: [batch_size, seq_len, d_model]\n",
        "    Returns:\n",
        "        x: [batch_size, seq_len, d_model]\n",
        "    \"\"\"\n",
        "    seq_len = x.size(1)\n",
        "    x = x + self.positional_encoding[:, :seq_len]\n",
        "    return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQ03LDPmWvf_"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int, n_heads: int):\n",
        "    super().__init__()\n",
        "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "    self.d_model = d_model\n",
        "    self.n_heads = n_heads\n",
        "    self.d_k = d_model // n_heads\n",
        "\n",
        "    self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "    self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "  def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        Q: [batch_size, n_heads, q_seq_len, d_k]\n",
        "        K, V: [batch_size, n_heads, k_seq_len, d_k]\n",
        "        mask: [batch_size, 1, 1, src_len] or [batch_size, 1, tgt_len, tgt_len]\n",
        "    Returns:\n",
        "        outp: [batch_size, n_heads, q_seq_len, d_k]\n",
        "    \"\"\"\n",
        "    d_k = Q.size(-1)\n",
        "\n",
        "    outp = torch.matmul(Q, K.transpose(-2, -1))\n",
        "    outp = outp / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      outp = outp.masked_fill(mask == 0, -1e9)\n",
        "    outp = F.softmax(outp, dim=-1)\n",
        "\n",
        "    outp = torch.matmul(outp, V)\n",
        "\n",
        "    return outp\n",
        "\n",
        "  def forward(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        Q: [batch_size, q_seq_len, d_model]\n",
        "        K, V: [batch_size, k_seq_len, d_model]\n",
        "        mask: [batch_size, n_heads, q_seq_len, k_seq_len]\n",
        "    Returns:\n",
        "        outp: [batch_size, q_seq_len, d_model]\n",
        "    \"\"\"\n",
        "    batch_size = Q.size(0)\n",
        "\n",
        "    Q = self.W_q(Q)\n",
        "    K = self.W_k(K)\n",
        "    V = self.W_v(V)\n",
        "\n",
        "    Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "    V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    outp = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "    outp = outp.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "\n",
        "    outp = self.W_o(outp)\n",
        "\n",
        "    return outp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hpsroSJPp1r"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float=0.1):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff, bias=True)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model, bias=True)\n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: [batch_size, seq_len, d_model]\n",
        "    Returns:\n",
        "        outp: [batch_size, seq_len, d_model]\n",
        "    \"\"\"\n",
        "\n",
        "    outp = F.relu(self.linear1(x))\n",
        "    outp = self.linear2(outp)\n",
        "\n",
        "    return outp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnP_duBPT_IK"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float=0.1):\n",
        "    super().__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: [batch_size, src_len, d_model]\n",
        "        src_mask: [batch_size, 1, src_len, src_len]\n",
        "    Returns:\n",
        "        outp: [batch_size, src_len, d_model]\n",
        "    \"\"\"\n",
        "    outp = self.mha(x, x, x, src_mask)\n",
        "    outp = self.dropout(outp)\n",
        "    outp = outp + x\n",
        "    outp = self.norm1(outp)\n",
        "\n",
        "    res = outp\n",
        "    outp = self.ff(outp)\n",
        "    outp = self.dropout(outp)\n",
        "    outp = outp + res\n",
        "    outp = self.norm2(outp)\n",
        "\n",
        "    return outp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PV77qXotPB6f"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, vocab_size: int, dropout: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "  def forward(self, x: torch.Tensor, src_mask: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: [batch_size, src_len]\n",
        "        src_mask: [batch_size, 1, src_len, src_len]\n",
        "    Returns:\n",
        "        outp: [batch_size, src_len, d_model]\n",
        "    \"\"\"\n",
        "    outp = self.embedding(x)\n",
        "    outp = self.pos_encoding(outp)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      outp = layer(outp, src_mask)\n",
        "\n",
        "    return outp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgKSU3gsg7wr"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, n_heads)\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.mha2 = MultiHeadAttention(d_model, n_heads)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "    self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self, x: torch.Tensor, encoder_outp: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: [batch_size, tgt_len, d_model]\n",
        "        encoder_output: [batch_size, src_len, d_model]\n",
        "        src_mask: [batch_size, 1, 1, src_len]\n",
        "        tgt_mask: [batch_size, 1, tgt_len, tgt_len]\n",
        "    Returns:\n",
        "        outp: [batch_size, tgt_seq_len, d_model]\n",
        "    \"\"\"\n",
        "\n",
        "    outp = self.mha1(x, x, x, tgt_mask)\n",
        "    outp = self.dropout(outp)\n",
        "    outp = outp + x\n",
        "    outp = self.norm1(outp)\n",
        "\n",
        "    res = outp\n",
        "    outp = self.mha2(outp, encoder_outp, encoder_outp, src_mask)\n",
        "    outp = self.dropout(outp)\n",
        "    outp = outp + res\n",
        "    outp = self.norm2(outp)\n",
        "\n",
        "    res = outp\n",
        "    outp = self.ff(outp)\n",
        "    outp = self.dropout(outp)\n",
        "    outp = outp + res\n",
        "    outp = self.norm3(outp)\n",
        "\n",
        "    return outp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08p0ZshEhL0J"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, n_layers: int, d_model: int, n_heads: int, d_ff: int, vocab_size: int, dropout: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "    self.pos_encoding = PositionalEncoding(d_model)\n",
        "\n",
        "    self.layers = nn.ModuleList([\n",
        "        DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
        "    ])\n",
        "\n",
        "    self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "    self.linear.weight = self.embedding.weight # weight sharing\n",
        "\n",
        "  def forward(self, x: torch.Tensor, encoder_outp: torch.Tensor, src_mask: torch.Tensor, tgt_mask: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        x: [batch_size, tgt_len]\n",
        "        encoder_outp: [batch_size, src_len, d_model]\n",
        "        src_mask: [batch_size, 1, 1, src_len]\n",
        "        tgt_mask: [batch_size, 1, tgt_len, tgt_len]\n",
        "    Returns:\n",
        "        outp: [batch_size, tgt_len, vocab_size]\n",
        "    \"\"\"\n",
        "    outp = self.embedding(x)\n",
        "    outp = self.pos_encoding(outp)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      outp = layer(outp, encoder_outp, src_mask, tgt_mask)\n",
        "\n",
        "    outp = self.linear(outp)\n",
        "\n",
        "    return outp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGjgBrSWN1OF"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, n_encoder_layers: int, n_decoder_layers: int, d_model: int,\n",
        "               n_heads: int, d_ff: int, vocab_size: int, dropout: int,\n",
        "               pad_id: int, bos_id: int, eos_id: int):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pad_id = pad_id\n",
        "    self.bos_id = bos_id\n",
        "    self.eos_id = eos_id\n",
        "\n",
        "    self.encoder = Encoder(n_encoder_layers, d_model, n_heads, d_ff, vocab_size, dropout)\n",
        "    self.decoder = Decoder(n_decoder_layers, d_model, n_heads, d_ff, vocab_size, dropout)\n",
        "\n",
        "  def get_attn_pad_mask(self, k_seq: torch.Tensor, pad_id: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        k_seq: [batch_size, k_seq_len]\n",
        "    Returns:\n",
        "        mask: [batch_size, k_seq_len]\n",
        "    \"\"\"\n",
        "    mask = (k_seq != pad_id)\n",
        "    return mask\n",
        "\n",
        "  def get_attn_look_ahead_mask(self, k_seq: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        k_seq: [batch_size, k_seq_len]\n",
        "    Returns:\n",
        "        mask: [batch_size, k_seq_len, k_seq_len]\n",
        "    \"\"\"\n",
        "    batch_size, k_seq_len = k_seq.size()\n",
        "    mask = torch.tril(torch.ones(k_seq_len, k_seq_len, dtype=torch.bool, device=k_seq.device)).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "    return mask\n",
        "\n",
        "  def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        src: [batch_size, src_seq_len]\n",
        "        tgt: [batch_size, tgt_seq_len]\n",
        "    Returns:\n",
        "        decoder_outp: [batch_size, tgt_seq_len, vocab_size]\n",
        "    \"\"\"\n",
        "\n",
        "    src_mask = self.get_attn_pad_mask(src, self.pad_id)\n",
        "    tgt_mask = self.get_attn_look_ahead_mask(tgt) & self.get_attn_pad_mask(tgt, self.pad_id).unsqueeze(1)\n",
        "\n",
        "    src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n",
        "    tgt_mask = tgt_mask.unsqueeze(1)\n",
        "\n",
        "    encoder_outp = self.encoder(src, src_mask)\n",
        "    decoder_outp = self.decoder(tgt, encoder_outp, src_mask, tgt_mask)\n",
        "\n",
        "    return decoder_outp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8ZDJzMZchhN"
      },
      "outputs": [],
      "source": [
        "model = Transformer(n_encoder_layers, n_decoder_layers, d_model, n_heads, d_ff, vocab_size, dropout, pad_id, bos_id, eos_id)\n",
        "model.to('cuda')\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1, betas=(0.9, 0.98), eps=1e-09)\n",
        "\n",
        "lr_lambda = lambda step: d_model**(-0.5) * min((step+1)**(-0.5), (step+1) * warmup_steps**(-1.5))\n",
        "\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iy608C-dIeo"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(count_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_FiJR29U9-5"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=new_tokenizer, padding=True)\n",
        "\n",
        "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
        "val_dataloader = DataLoader(tokenized_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
        "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_decoder_input(tgt: torch.Tensor, pad_id: int, eos_id: int) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "      tgt: [batch_size, tgt_len]\n",
        "  Returns:\n",
        "      decoder_input: [batch_size, tgt_len]\n",
        "  \"\"\"\n",
        "\n",
        "  # Remove </s>\n",
        "  decoder_input = tgt.detach().clone()\n",
        "  decoder_input[decoder_input == eos_id] = pad_id\n",
        "  return decoder_input\n",
        "\n",
        "def preprocess_decoder_label(tgt: torch.Tensor, pad_id: int) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Args:\n",
        "      tgt: [batch_size, tgt_len]\n",
        "  Returns:\n",
        "      decoder_label: [batch_size, tgt_len]\n",
        "  \"\"\"\n",
        "\n",
        "  # Remove <s>\n",
        "  decoder_label = tgt[:, 1:]\n",
        "  pad_tensor = torch.full((tgt.size(0), 1), pad_id, device=tgt.device)\n",
        "  decoder_label = torch.cat([decoder_label, pad_tensor], dim=1)\n",
        "\n",
        "  # Replace <pad> with negative value\n",
        "  decoder_label[decoder_label == pad_id] = -100\n",
        "  return decoder_label"
      ],
      "metadata": {
        "id": "xWi0kJch1t0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "start_epoch = 0\n",
        "best_loss = np.inf\n",
        "patience = 3\n",
        "counter = 0\n",
        "wandb_run_id = None\n",
        "\n",
        "# Load checkpoint if it exists\n",
        "checkpoint_path = None # os.path.join(colab_dir, 'transformer-from-scratch/modelXXX.pt')\n",
        "if checkpoint_path:\n",
        "\n",
        "  print(\"Loading checkpoint...\")\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "  start_epoch = checkpoint['epoch'] + 1\n",
        "  best_loss = checkpoint['best_loss']\n",
        "  wandb_run_id = checkpoint['wandb_run_id']\n",
        "\n",
        "  print(f\"Resuming training from epoch {start_epoch}\")"
      ],
      "metadata": {
        "id": "erBM9zLq4CpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write to wandb\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "wandb.login(key=wandb_api_key)\n",
        "wandb.init(\n",
        "    project=\"transformer-from-scratch\",\n",
        "    config=hyperparameters,\n",
        "    id=wandb_run_id,\n",
        "    resume=\"must\" if wandb_run_id else None,\n",
        ")"
      ],
      "metadata": {
        "id": "5Yvyt9Q6-cIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KDcneV1blxA"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "for epoch in range(start_epoch, n_epochs):\n",
        "\n",
        "  model.train()\n",
        "  total_train_loss = 0\n",
        "\n",
        "  train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{n_epochs} [Train]\")\n",
        "\n",
        "  for batch in train_progress_bar:\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    src = batch['input_ids'].to('cuda')\n",
        "    tgt = batch['labels'].to('cuda')\n",
        "    decoder_input = preprocess_decoder_input(tgt, pad_id, eos_id)\n",
        "    decoder_label = preprocess_decoder_label(tgt, pad_id)\n",
        "\n",
        "    decoder_outp = model(src, decoder_input)\n",
        "\n",
        "    loss = F.cross_entropy(decoder_outp.view(-1, decoder_outp.size(-1)), decoder_label.view(-1), ignore_index=-100)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    total_train_loss += loss.item()\n",
        "    train_progress_bar.set_postfix(train_loss=loss.item())\n",
        "\n",
        "    wandb.log({\n",
        "            \"train_batch_loss\": loss.item(),\n",
        "            \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "  avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "  model.eval()\n",
        "  total_val_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "\n",
        "    val_progress_bar = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{n_epochs} [Validation]\")\n",
        "\n",
        "    for batch in val_progress_bar:\n",
        "      src = batch['input_ids'].to('cuda')\n",
        "      tgt = batch['labels'].to('cuda')\n",
        "      decoder_label = preprocess_decoder_label(tgt, pad_id)\n",
        "\n",
        "      decoder_outp = model(src, tgt)\n",
        "\n",
        "      loss = F.cross_entropy(decoder_outp.view(-1, decoder_outp.size(-1)), decoder_label.view(-1), ignore_index=-100)\n",
        "\n",
        "      total_val_loss += loss.item()\n",
        "      val_progress_bar.set_postfix(val_loss=loss.item())\n",
        "\n",
        "  avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "\n",
        "  wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"avg_train_loss\": avg_train_loss,\n",
        "        \"avg_val_loss\": avg_val_loss\n",
        "    })\n",
        "\n",
        "  # Early Stopping\n",
        "  if avg_val_loss < best_loss:\n",
        "    best_loss = avg_val_loss\n",
        "    counter = 0\n",
        "    print(f\"Validation loss improved.\")\n",
        "\n",
        "  else:\n",
        "    counter += 1\n",
        "    print(f\"Validation loss did not improve. Counter: {counter}/{patience}\")\n",
        "\n",
        "  # Save latest checkpoint\n",
        "  print(\"Saving latest checkpoint...\")\n",
        "  checkpoint = {\n",
        "      'epoch': epoch,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'scheduler_state_dict': scheduler.state_dict(),\n",
        "      'best_loss': best_loss,\n",
        "      'wandb_run_id': wandb.run.id,\n",
        "  }\n",
        "  torch.save(checkpoint, os.path.join(colab_dir, f\"model{epoch:03d}.pt\"))\n",
        "\n",
        "  if counter >= patience:\n",
        "    print(\"Early stopping triggered.\")\n",
        "    break\n",
        "\n",
        "  print(f\"Epoch: {epoch+1}, Avg Train Loss: {avg_train_loss:.4f}, Avg Val Loss: {avg_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_path = None # os.path.join(colab_dir, 'transformer-from-scratch/modelXXX.pt')\n",
        "if best_model_path:\n",
        "  model.load_state_dict(torch.load(best_model_path))"
      ],
      "metadata": {
        "id": "g4LFVUX6kaqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLvNAW6HELtI"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "bleu_score = BLEUScore()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    test_progress_bar = tqdm(test_dataloader, desc=\"[Test]\")\n",
        "\n",
        "    for batch in test_progress_bar:\n",
        "\n",
        "        src = batch['input_ids'].to('cuda')\n",
        "        tgt = batch['labels'].clone()\n",
        "        tgt[tgt == -100] = pad_id\n",
        "        tgt = tgt.to('cuda')\n",
        "\n",
        "        # Start to generate french sentences with <s>\n",
        "        decoder_input = torch.full((src.shape[0], 1), bos_id, device=src.device)\n",
        "\n",
        "        for _ in range(max_len + 1):\n",
        "\n",
        "            decoder_outp = model(src, decoder_input)\n",
        "            next_token = decoder_outp.argmax(dim=-1)[:, -1].unsqueeze(1)\n",
        "            decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "\n",
        "            if (next_token == eos_id).all():\n",
        "                break\n",
        "\n",
        "        pred_text = new_tokenizer.batch_decode(decoder_input, skip_special_tokens=True)\n",
        "        ref_text = new_tokenizer.batch_decode(tgt, skip_special_tokens=True)\n",
        "        src_text = new_tokenizer.batch_decode(src, skip_special_tokens=True)\n",
        "\n",
        "        # Print one example per batch\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Source:      {src_text[0]}\")\n",
        "        print(f\"Reference:   {ref_text[0]}\")\n",
        "        print(f\"Prediction:  {pred_text[0]}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        predictions.extend(pred_text)\n",
        "        # Wrap references of each prediction with list as BLEU can have multiple references\n",
        "        references.extend([[r] for r in ref_text])\n",
        "\n",
        "bleu = bleu_score(predictions, references)\n",
        "print(f\"\\nTest BLEU Score: {bleu.item() * 100:.2f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}